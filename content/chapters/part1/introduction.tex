%% Introduction
%%=========================================

\chapter{Introduction and Background}
\label{ch:introduction}
We find ourselves in the middle of the digital age. As the industrial revolution gave us new manufacturing processes at the middle of the seventeenth century, the digital revolution gave us digital electronics, most noticeable the computer \citep{freeman2001time}. With the arrival of high-performance computers and high speed networks, use of digital technologies increased rapidly. The increase in storage capacities gave us the possibilities to store lots of information digitally for preservation. Digital technologies enable information to be created, manipulated, disseminated, located, and stored with increasing ease \citep{lee2002state}. In addition, digital storage usually takes up less physical space compared to the analogue counterpart, and its storage medium is usually cheaper \citep{morris2003evolution}. Although hard drives also are exposed to ``wear and tear" that deteriorate it, analogue information, such as a book, usually deteriorate much faster. Due to these benefits, a lot data that used to be analogue finds new life in digital formats. Photos, audio, video, and books are just a few types of data that are commonly found digitally these days.

\citep{misc-oed-digitization} defines the action or process of digitizing, the task of converting analogue data into digital data, as ``digitization". One prime example of mass digitization is Google Inc.\footnote{\url{https://www.google.com/intl/en/about/}} and their service Google Books\footnote{\url{https://books.google.com}}. Google Books is a service that provides full text of books and magazines online. Other similar projects like Google Books include JSTOR\footnote{\url{https://www.jstor.org}}, which has digitized nearly a thousand journals, dating back to the mid 19th century, and Bokhylla.no\footnote{\url{http://bokhylla.no}} (The Bookshelf). Bokhylla.no is a project initiated by the National Library of Norway. It was launched in 2009 and aims to provide online access to literature published in Norwegian. The service will contain about 250 000 books when it is completed in 2017 \citep{misc-nb-digial-library}.

Google has made their goal with Google Books very clear, it is primarily a search service, which also allows some viewing of the context of the search terms. They do not attempt to provide a reading environment, and consider themselves first and foremost a index of books and published works \citep{coyle2006mass}. Bokhylla.no on, the other hand, provides enhanced reading environments, where the visitor can read entire books from cover to cover. The service also provides in-text search of the entire library. 

Despite their different approaches to digitalized reading, they utilize the same types of technologies. While simply scanning the books will suffice to make the literature available online, other technologies are needed to actually index the content. Indexing is the process of capturing the scanned text and converting it into editable and searchable data. To capture this data, a technology called Optical Character Recognition, or OCR for short, is used. OCR has many applications, and is in use in many areas today. Book scanning, number plate recognition, handling of checks, passports, as well as assistive technologies for blind and visually impaired users may use some kind of OCR \citep{mori1999optical, kurzweil2000reading}.

%%=========================================

\section{Problem and Motivation}
\label{sec:problem_motivation}
OCR is, in a broad sense, a branch of artificial intelligence. In addition to artificial intelligence, OCR is also a research branch in pattern recognition and computer vision \citep{mori1999optical}. It was first believed that it would be easy to develop an OCR, and researchers estimated that an accurate reading machine would be introduced in the 1950s. During the 1950s and the early 1960s researchers were still struggling with an ideal OCR model. Their research has since laid the foundation for modern research in the field \citep{mori1992historical}.

We have come a long way since the 1950s, and \citep{ye2015text} states:

\begin{quote}
    While many researchers view optical character recognition (OCR) as a solved problem, text detection and recognition in imagery possess many of the same hurdles as computer vision and pattern recognition problems driven by lower quality or degraded data.
\end{quote}

Under optimal conditions, certain OCR systems can perform very well. OCR has yielded great results in clean and well-formatted documents, in some cases achieving up to 99\% recognition rates. In cases where the documents are not clean and well-formatted, with an OCR facing variations of text layouts and fonts, uneven illuminations, and other obstacles, the challenge is much greater, and the systems have a much lower rates of detection and recognition \citep{ye2015text}.

%%\red{In 2012 it became clear that Posten Norge's\footnote{Nordic mail and logistics group} multimillion OCR software failed to read the font Times New Roman, which is one of the most common computer fonts. Letters from LÃ¥nekassen (Norwegian State Educational Loan Fund), had to be manually sorted until they got their typeface switched to one that the OCR software could recognize \citep{misc-aftenposten-posten}. This illustrates the point that despite how well some of these systems works, they may fail under certain conditions.}

This has led us to believe that the problem of OCR is not solved, and there is still room for improvements and experimentation with new approaches. One of the area that still have a lot of room for improvement is recognizing characters that are damaged or obfuscated. Text can be damaged for a number of reasons, such as pasting of paper, ink spreading, fading, or dirt \citep{bhardwaj2014imaging}. See Figure \ref{fig:damaged-text} for example of text that is damaged and missing a lot of its original data. In this master thesis we present a new way to recognize damaged letters by using their ``signature".

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/chapter1/damaged.png}
    \caption{Badly damaged text missing a lot of its original data}
    \label{fig:damaged-text}
\end{figure}

%%=========================================

\section{Goals and Research Questions}
\label{sec:goals_and_research_questions}
Our goal is to use the ``signature" of a character to classify it. We can imagine a ``signature" by applying two masks to the original image. These masks spans the total width of the image. The masks also cover most of the image from the top to bottom, and from the bottom to the top. The only area that is not covered by the masks is a thin line. This line defines the ``signature" for our image. In Figure \ref{fig:thesis-signature} we have a image with the word ``THESIS". The original characters have a height of 50 pixels, and our masks are applied at both the top and bottom of the image. The areas that are covered with masks are grayed out, and the only exposed part of the image is a line that has a height of one pixel. This line is a ``signature" of the original image. By applying these masks, we now omit the data and information that is covered by these.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/chapter1/signature.png}
    \caption{Illustration of a word with a signature with a height of one pixel}
    \label{fig:thesis-signature}
\end{figure}

Naturally, this approach has its pros and cons. First of all, the data in the signature is essential. This approach can not handle any damage in the data within the signature. This means that this approach is useless if there is no one single line that is whole. In real life scenarios, this may be a very real concern, but for the sake of researching and analyzing an approach like this, we can avoid the problem in our constructed data sets. The pros of the ``signature approach" is that the data outside the signature is irrelevant, and we only use a very small subset of the original information. This means that as long as the constraints explained above holds, we can deal with very damaged and obfuscated data.

The overarching goal of this master thesis is:

\begin{description}
    \item[Goal]{\textit{Can machine learning use signature sequences to correctly predict the original characters in a word?}}
\end{description}

To expand on the goal stated above, we also have the following research questions that we hope to answer during the work on this master thesis.

\begin{description}
    \item[Research Question 1]{\textit{Does ambiguity in character signature sequences impact recognition rates?}}
    \item[Research Question 2]{\textit{Does different fonts, font characteristics, font styles, and other typesetting factors affect recognition rates?}}
    \item[Research Question 3]{\textit{Can a signature sequence based model handle multiple fonts at the same time?}}
\end{description}

With an approach like the one we present, there are many questions that can be asked about the potential model. Trying to make the research done in this master thesis as valuable as possible, we have focused our research questions on areas that are relevant for further research. Instead of focusing on narrow and niche questions, we have focused on more general and overarching questions. The research questions and their meaning are expanded on in section \ref{sec:research_questions_and_approach}.

%%=========================================

\section{Research Method}
Given the research goal and questions presented in the previous section, the most meaningful research method would be to develop a model in an attempt to answer them. This prototype model should be used through experiments and the relevant results should be evaluated in terms of our goals and questions. The model should be premised on relevant theory and state of the art technologies relevant for our problem. The research method and approach is presented in detail in Chapter \ref{ch:methodology}.

%%=========================================

\section{Contributions}
The contribution of this master thesis is mainly exploration of how to use machine learning and signature sequences to do predictions. The results of the experiments and conclusions done in this master thesis may lay the foundation of new way to use machine learning to recognize patterns in data using sequences. Although the ideas presented in this master thesis is not new, they mind prove that certain problems may be solvable using data in new ways.

%%=========================================

\section{Thesis Structure}
\noindent
\begin{minipage}{\linewidth}
    \textbf{Part I - Introduction and Methodology}
    \begin{itemize}
        \item\textbf{Introduction} contains a rough description of the overarching problem, and is a general introduction for this master thesis.
        \item\textbf{Methodology} establishes the methodology used throughout the master thesis. It explains how the research should be carried out and why we chose the strategies we used.
    \end{itemize}
\end{minipage}

\vspace{0.5cm}\noindent
\begin{minipage}{\linewidth}
    \textbf{Part II - Establishing the State of the Art}
    \begin{itemize}
        \item\textbf{Problem Elaboration} \red{TODO}
        \item\textbf{Background Theory} \red{TODO}
        \item\textbf{Related Work} \red{TODO}
    \end{itemize}
\end{minipage}

\vspace{0.5cm}\noindent
\begin{minipage}{\linewidth}
    \textbf{Part III - Model and Experiments}
    \begin{itemize}
        \item\textbf{Development Process} \red{TODO}
        \item\textbf{Architecture} \red{TODO}
        \item\textbf{Experiments and Results} \red{TODO}
    \end{itemize}
\end{minipage}

\vspace{0.5cm}\noindent
\begin{minipage}{\linewidth}
    \textbf{Part IV - Discussion and Conclusion}
    \begin{itemize}
        \item\textbf{Evaluation} \red{TODO}
        \item\textbf{Conclusion} \red{TODO}
    \end{itemize}
\end{minipage}
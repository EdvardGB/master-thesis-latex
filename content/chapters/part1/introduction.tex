%% Introduction
%%=========================================

\chapter{Introduction}
\label{ch:introduction}
We find ourselves in the middle of the digital age. As the industrial revolution gave us new manufacturing processes at the middle of the seventeenth century, the digital revolution gave us digital electronics, most noticeable the computer \citep{freeman2001time}. Along with computers came the possibilities to store information digitally \red{cite}. Storing data digitally has many benefits compared to analogue storage. Digital data takes less space, is usually cheaper, and you can more easily duplicate, transfer, and process the data \red{cite}. Due to these benefits, a lot data that used to be analogue finds new life in digital formats. Photos, audio, video, and books are just a few types of data that are commonly found digitally these days.

%%=========================================

\section{Background}
\citep{misc-oed-digitization} defines the action or process of digitizing, the task of converting analogue data into digital data, as ``digitization". In many areas digitization is a process that has been going on for decades. Now that we more often than not assume data is available digitally, countless initiatives have taken it upon themselves to provide digital data that was previously only available in analogue formats. One such project is Bokhylla.no (The bookshelf). Bokhylla.no is a project initiated by the National Library of Norway. It was launched in 2009 and aims to provide online access to literature published in Norwegian. The service will contain about 250 000 books when it is completed in 2017 \citep{misc-nb-digial-library}.

While simply scanning the books will suffice to make the literature available online, other technologies are needed to actually index the content. By indexing, we mean the process of capturing the scanned text and converting it into editable and searchable data. To capture the data, we use a technology called Optical Character Recognition, or OCR for short. OCR is used to convert images of text to machine-encoded text. OCR has many applications, and is in use in many areas today. Book scanning, number plate recognition, handling of checks, passports, as well as assistive technologies for blind and visually impaired users all use some kind of OCR.

%%=========================================

\section{Problem and Motivation}
\label{sec:problem_motivation}
OCR, as described in the previous section, is an incredible broad area, and an astounding amount of research, approaches and technological solutions are already written and developed in the field. As with most models and algorithms, they have a specific ``area of use". Certain models works better than other under some conditions, and worse under others. There is no ``perfect" OCR model that can solve any trivial OCR related problem. While some generic approaches exists, most are to some degree customized to find a solution given a predefined set of constraints.

Problems arise once we try to give a model data that it is outside the predefined set of constraints. The data can either be encoded incorrectly, or simply too damaged for our model to handle. If the data is too damaged, it means the input is too obfuscated for the model to recognize the underlying data. As OCR usually deals with analogue data, coping with damaged or altered data it something models usually have to do. Despite this, there is always a limit as to what is simply ``too damaged". Figure \ref{fig:damaged-text} illustrates a word that has been badly damaged. With over 67\% of its original data lost, can a typical classifier still correctly classify each character?

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/chapter1/damaged.png}
    \caption{Badly damaged text missing a lot of its original data}
    \label{fig:damaged-text}
\end{figure}

The motivation behind this thesis is to see if it is possible to do character recognition using only a very limited subset of the original character data. For example, if we have a image of a single letter that is 50 pixels wide and 50 pixels high\footnote{This image would have a total of $50 \cdot 50 = 2,500$ pixels.}, is it possible to recognize the letter if we only have 500 pixels available? What about 250? Or 100? If we are able to archive good results in this type of prediction, we might be able to recognize letters, words and texts where a lot of the data is lost or damaged. Trying to find new ways to do character recognition is the main motivation behind this thesis.

%%=========================================

\section{Goals and Research Questions}
\red{Unfinished section}
\label{sec:goals_and_research_questions}
Our goal is to use the ``signature" of a character to classify it. We can imagine a ``signature" by applying one or more masks to the original image that spans the total width of the image. The mask also covers most of the image from the top to bottom, exposing only a small part from original image. Figure \ref{fig:thesis-signature} we have a image with the word ``THESIS". The original characters have a height of 50 pixels, and our masks are applied at both the top and bottom of the image. The areas that are covered with masks are grayed out, and the only exposed part of the image is a line that has a height of one pixel. This line is a ``signature" of the original image. By applying these masks, we now omit the data and information that is covered by these.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/chapter1/signature.png}
    \caption{Illustration of a word with a signature with a height of one pixel}
    \label{fig:thesis-signature}
\end{figure}

Classification using signatures is a harder problem than classification using the original and whole available data. We have less information, and our input has introduced more uncertainty. We also introduce a problem that may or may not contain ambiguous data, as further explained in Chapter \ref{ch:problem}. The generalized goal of this thesis is:

\begin{description}
\item[Goal] {\textit{Can machine learning predict letters and words in a ambiguous problem space}}
\end{description}

The idea is to use deep learning architectures that are not commonly used to do image recognition. 

\input{content/various/research_questions}

%%=========================================

\section{Research Method}
\red{Unfinished section}
The thesis is centered around the development of model. We first establish what kind of model makes sense for our problem. Next we find out what similar models exists, and what is the current state of the art. With the relevant background information, we develop our own model. The model is then tested and evaluated. A more detailed description of our research method and approach is presented in Chapter \ref{ch:methodology}.

%%=========================================

\section{Contributions}
The contributions of this thesis is mainly exploration of how to combine sequence classification and image recognition. The results of the experiment presented in this thesis will may lay the foundation of new way to use machine learning to recognize patterns in data.

Sequence classification has been used with promising results in areas such as natural language translation, speech recognition, and computer vision. Similarly, various classes and types of artificial neural networks have yielded exceptional good results in the area of image and pattern recognition. We will try to combine these two techniques, and analyze just how well our model behaves.

\red{Expand on this(?)}

%%=========================================

\section{Thesis Structure}
\red{Unfinished section}
In the next chapter, Chapter \ref{ch:problem}, we will present the problem in greater detail. These details will help understand what problem we are trying to solve. In Chapter \ref{ch:background} we present the related background theory as well as motivation behind this thesis. This chapter will expand on \red{something}. In Chapter \ref{ch:architecture} we define our architecture, and in Chapter \ref{ch:experiments_and_results} we present the experiments as well as recorded results. We will evaluate these results in Chapter \ref{ch:evaluation}, and in the closing Chapter \ref{ch:conclusion} we will try to draw a conclusion from the results, as well as look at potential future work.
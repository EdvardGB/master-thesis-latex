%% Development Process
%%=========================================

\chapter{Development Process}
\label{ch:development_process}
This chapter presents how the development process of our model was conducted. The process is briefly summarized, and is mainly done to illustrate how we used our research methodology to develop our models as a product of working iteratively. Note that this chapter does not present any details about the models architecture, this is done in the next chapter. Also note that because the iteration process not necessarily is followed in a strict manner, iterations that led nowhere are ignored, and some work that was carried out in-between iterations are moved where they fitted best. Some details and work is also omitted, for the sake of having a clear and brief summarizing.

%%=========================================

\section{First Iteration -- Ground Work And Exploration}
The first iteration in the cycle included a lot ground work and some exploration. The work done in this iteration was mostly for exploration to back up claims from the preliminary research phase.

\subsection{Construction Pipeline}
\label{sec:construction_pipeline}
Because we had no data set available to train, validate, and test our model on, this had to be created manually. Creating these sets were done by implementing a configurable pipeline system that involved processing over several steps:

\begin{enumerate}
    \item Build a complete dictionary of words, make sure they only include allowed characters.
    \item Select random words from the list and construct training, validation, and test sets. Make sure no words are picked that exceeds the maximum word length configuration. Remove duplicated words both across lists, and within the same list as per configured.
    \item Using an image library, write the text from the lists on individual (empty) canvases, with the specified font type and font size.
    \item Find the boundaries of the characters and crop the text to remove the excess space on the canvas. The boundaries needs to be the same across canvases, so characters have the same baseline location in all of them.
    \item As configured, apply the masks and extract the signature values.
    \item Save the signature values for later.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/development_process/pipeline.pdf}
    \caption{Illustration of pipeline process from canvas to signature}
    \label{fig:development-pipeline}
\end{figure}

Figure \ref{fig:development-pipeline} illustrates the steps described above. The leftmost image is the text written on a big canvas. This canvas has a predefined size, as we are unable to predict the actual size of text before it is written out with our image library. This canvas is then cropped based on the boundaries calculated by the individual characters. As we can see in the middle image, not all the characters are ``standing" on the image outline, which is a result of how our image library handles writing text. The rightmost image is the extracted signature from the middle image. The signatures actual height is one pixel, and is extracted fourteen pixels from the bottom. It was expanded vertically for increased visibility. The signature sequence is then extracted by iterating over the pixels in the image, and the final sequence is stored for later. The sequence for the word ``AMPLITUDES", which is ten letters long, has a length of 31.

\subsection{Transformation system}
The process explained above only needed to be executed every time we wanted new data sets. Usually there was no need to create new sets and the same words and signatures could be reused. However, during development of various models, their expected input and output formats varied. Another system was created to transform the signatures from the previous pipeline into the expected input and output formats. This system, called the ``transformator", accepted a list of handlers that were executed in sequence. Each handler did a modification to either the input or the output format, and the data was propagated to the next handler. This way, the handlers could easily be reordered or swapped if a model expected another format. Typical tasks included padding the input to a given width, rescaling the input to whole integers, or turning the output into a one-hot matrix.

\subsection{First Model Experimentation}
After researching state of the art solutions in the area of machine translation, various sourced had made it very clear that LSTMs alone could not handle a sequence to sequence problem. \citep{sutskever2014sequence} stated in the description of their purposed model:

\begin{quote}
    The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships.
\end{quote}

In our particular problem, this alignment is not know, and this makes it difficult for our model to map input to output in a sensible way. One input of our problem does not necessarily map directly to an output. This creates an alignment difference between input and output where the input in most cases will be much wider than the output. This is exactly the problem pointed out by \citep{sutskever2014sequence}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/development_process/lstm-alignment.pdf}
    \caption{Alignment between input and output in a recurrent neural network}
    \label{fig:lstm-alignment}
\end{figure}

Figure \ref{fig:lstm-alignment} illustrates the difference in the alignment between input and output, reusing the data from Table \ref{eq:input_stop_words} without stop words. The alignment difference means that the first output of the model depends on the input it has yet to see, which the model can not know. The difficulties with the alignment made projecting output through a LSTM using its hidden state would be meaningless. 

Testing a LSTM model that simply projected its hidden states to the output gave very poor results. It was clear, both from theory and from results, that such an approach would not be suitable for our problem. We already knew that the encoder-decoder architecture did not have this problem, and we noted for the next iteration how to proceed with the development of our model. 

%%=========================================

\section{Second Iteration -- Projecting Vector}
The first iteration made it clear that simply projecting hidden states of LSTMs was not suitable for our problem. The second iteration used an approach that repeats a vector across time steps. This approach is architecturally not very different from an encoder-decoder, but lacks some of the characteristics that characterizes an encoder-decoder model.

This model, called {\tt RepVec}, short for ``repeat vector", can be seen as a ``poor man's encoder-decoder", as we will see in section yy. We decided to implement this model as it was sort of a middle ground between the LSTM approach from the previous iteration, and a complete encoder-decoder model. Implementing this solution was mostly done to see if we could solve the problem without investing in a complex system that relied on the encoder-decoder. Weather or not the model was successful, it would tell us how LSTMs could work with our problem if we removed the alignment complications from the equation.

%%=========================================

\section{Third Iteration -- Encoder-Decoders}
With the first model developed during the previous iteration, this iteration focused on developing fully fleshed out encoder-decoder models. We knew this would could likely be the best performing model based on the studies conducted earlier. All research pointed in the direction of an encoder-decoder model for a problem similar to ours. The main reason we had not skipped directly to testing one of these out what for the sake of the ``design and creation" and its iteratively approach to solving problems, as well as taking our time to explore possibilities. The earlier iterations had also given us valuable insights in the nature of our problem.

There were developed two separate models in this iteration. These were encoder-decoders with and without attention. These were named {\tt EncDecReg} and {\tt EncDecAtt} respectively, short for ``encoder-decoder regular" and ``encoder-decoder attention".

%%=========================================

\section{Fourth Iteration -- Parameter Adjustments}
The previous iteration saw the implementation of the two encoder-decoder models. We had also implemented a model based on repeating LSTM output before that. Some minor testing of these models had been carried up until this point, but this iteration was mostly dedicated to tuning the models for better performance. This was done by running bigger test and and exploring various configurations in ways we felt made sense. 

Despite the time we spent on these adjustments, there are bound to be ways to get the models to behave even better. The configurations we ended up with were not that different from the default options in the frameworks we used, and a lot time was spent to understand why things were configured like it was, not necessarily changing it completely. Our models also archived relatively good results on the tests we ran, so we saw no reason to dismantle the models completely to gain an insignificant improvement.

%%=========================================

\section{Fifth Iteration -- Finalization and Cleanup}
The fifth, and final iteration, saw the finalization of system. This included reimplementing some parts of the system that over the course of the development grew far too complex. While this did not benefit us directly as the work on the system was already completed, we wanted the system to be tidy and the code to following the coding standards.
%% Architecture
%%=========================================

\chapter{Architecture}
\label{ch:architecture}
As presented in Chapter \ref{ch:development_process}, we implemented three different models for our experiments. This chapter explains the underlying architecture and details of these models. Some of the details have also been explained in Chapter \ref{ch:background} and \ref{ch:related_work}.

%%=========================================

\section{Programming Language and Frameworks}
The entire project was implemented in Python. Python was chosen primarily because it has a number of great open-source software libraries for machine learning. It is also relatively fast and easy to write, which meant that implementing the supporting systems, such as the generation of data sets, should not be a problem.

Keras\footnote{\url{https://www.keras.io}} was initially used as the neural network API. It is a very simple wrapper around other back-ends such as Tensorflow\footnote{\url{https://www.tensorflow.org}} and Theano\footnote{\url{http://deeplearning.net/software/theano/}}. The simplicity of Keras made it easy to implement and experiment with various models. The earlier iterations of the development benefited immensely from the simplicity that Keras provided.

In later iterations, more advanced and customizable models were necessary, specifically models that used the encoder-decoder architecture. Keras had some add-ons that implemented this, but we struggled with it being incompatible with more recent versions of Keras and the back-ends. It also turned out that Tensorflow provided encoder-decoder solutions via their {\tt legacy\_seq2seq} contrib module. An option was to develop the encoder-decoder architecture as a Keras add-on ourselves, but as Tensorflow provided them almost out of the box, we decided to just use the Tensorflow modules. Because of this we have one model that is based on Keras (but uses the Tensorflow as its back-end), and two models that are implemented directly in Tensorflow.

%%=========================================

\section{Common Architecture}
Despite the three models being totally separated from each other, they have a few things in common. This section highlights some of the configurations that apply to all the models.

\subsection{Embedding}
All three models used embedding for the input. Embedding, also known as a type of vector space model, is a way to map vocabulary to vectors of real numbers. The concept of distributed representation for symbols dates back several decades \citep{hinton1986learning}, although the most common approaches usually follows a more modern approach \citep{bengio2003neural}. The goal of word embedding is to find a representation that did not suffer from \emph{curse of dimensionality}, problems that arise when organizing data in high-dimension space. Such representation is important in problems with a very big vocabulary. In our problem, the vocabulary is relatively small, and we used embedding most as a convenient way to represent our data.

As illustrated in section \ref{sec:evaluating_problem_input_and_output}, our input sequences were first encoded as whole integers, where negative valued ones indicated a series of black pixels, and a positive value indicated a series of white pixels. Because the embedding layer required whole integer index-values, the sequences were re-indexed so that their values started from \(0\) and increased by one to the final index which was the size of the vocabulary minus one (\(|V| - 1\)). This single value representation of each value in the sequence was then fed to the embedding. 

In vocabulary where relationships are defined, you may derive to a word by doing some mathematical operation on two or more other words, for example:  \(\vec{\text{king}}+\vec{\text{woman}}=\vec{\text{queen}}\). In our models, we let the weights for the embedding be initialized with an uniform distribution over the half-open interval \(\big[-\sqrt{3}, \sqrt{3}\big)\), essentially placing each value somewhere randomly in the vector space. This value was the default value in Tensorflow, and as the initial placement was irrelevant, we decided to stick with it.

Initializing the values randomly is a common practice using embedding where the relationships between the individual values are not pre-calculated. During training, the embedding layer is fully trainable, which means that the training algorithm may alter the weights associated with the embedding values, moving it around in the vector space. This way, the embedding representation for the values may be grouped closer, or moved further away from each other, defining some kind of relationship that is not explicitly expressed. The use of embedding gave a increase in overall accuracy while running our model, compared using a one-hot vector representation. This may be due to the increased express-ability that embedding gives over one-hot vectors, where no relationship between one value and another is defined. In all three models, the embedding is expressed as a vector with \(1024\) values.

\subsection{Configuration of the LSTMs}
Although Keras and Tensorflow have their own implementation of the LSTM unit, their implementations are both based on \citep{hochreiter1997long}. The inner activation function was hyperbolic tangent (see equation \ref{eq:function_tanh} and plot in Figure \ref{fig:plot_function_tanh}), as assumed in the explanation given in section \ref{sec:long_short_term_memory}. The activation function applied over the current time step was a regular sigmoid function, see equation \ref{eq:function_sigmoid} and plot in Figure \ref{fig:plot_function_sigmoid}. These configurations are more or less the default ones, although Keras by default uses a function called {\tt hard\_sigmoid} for its recurrent activation function, which is a linear approximation of the sigmoid function that is faster to compute. The Keras version of the LSTM was changed to use the regular sigmoid function. Other than that, the default function were kept as we saw, or found, no reason or benefit of changing this.

\begin{figure}[ht]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_tanh}
            \tanh{x} = \frac{\sinh{x}}{\cosh{x}} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
        \end{equation}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_sigmoid}
            \frac{1}{1+e^{-x}}
        \end{equation}
    \end{minipage}%
\end{figure}

\begin{figure}[ht]
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-5.0, xmax=5.0,
                    ymin=-1.5, ymax=1.5,
                    minor y tick num={5},
                    minor x tick num={3},
                    axis lines=center,
                    axis on top=true,
                    domain=-5:5,
                    ylabel=$y$,
                    xlabel=$x$,
                ]
                    \addplot[mark=none,draw=red,ultra thick]{tanh(\x)};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted hyperbolic tangent function}
        \label{fig:plot_function_tanh}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-6, xmax=6,
                    ymin=0, ymax=1,
                    xtick={-5, -3, -1, 1, 3, 5},
                    ytick={0, 0.5, 1},
                    axis lines=center,
                    axis on top=true,
                    domain=-6:6,
                    ylabel=$y$,
                    xlabel=$x$
                ]
                    \addplot[mark=none,draw=red,ultra thick]{1/(1+exp(-\x))};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted sigmoid function}
        \label{fig:plot_function_sigmoid}
    \end{minipage}%
\end{figure}

The forget gate in the LSTM module was initialized to \(1.0\). This is done in order to reduce the scale of forgetting at the beginning of training, and is something that is recommended in \citep{zaremba2015empirical}. This value also defaults to \(1.0\) in both Keras and Tensorflow.

\subsubsection{Grouping of LSTMs}
The LSTMs were, in all three models, used in groups. Testing done on a trial-and-error basis during development showed that deeper LSTMs gave better performance than shallower ones. However, a depth greater than three usually did not impact the results noticeably. Because of that we always use our LSTMs in groups of three. This is further specified in the architecture for each model.

\subsubsection{Dropout}
To avoid overfitting, we utilized dropout in our network. Dropout works by randomly dropping units, along with their weights, by setting their values to \(0\) \citep{srivastava2014dropout}. The dropout mechanism is only used during training, to avoid overfitting from the training set, and skipped during validation and testing.

\citep{gal2016theoretically} purposed a another way of adding dropout to a recurrent neural network. Their purposed alternative applies the same dropout mask over every time step. The {\tt EncDecReg} and {\tt EncDecAtt} model uses this alternative dropout mechanic as Tensorflow offered native support for the implementation.

The dropout rate after \emph{each} LSTM was \(0.2\), which meant that around \(20\%\) of all units were ignored after each individual LSTM unit.

\subsection{Initializing of Weights}
\red{Unfinished section}

%%=========================================

\section{Accuracy Metric}
Calculating our accuracy was done by a method {\tt categorical\_accuracy} in Keras. A similar metric was implemented in Tensorflow, and is implemented in Example \ref{alg:categorical_accuracy}.

The method works by iterating over the correct labels and the final predictions, and compare the index with the highest values in each list. The labels would be a one-hot vector where the {\tt argmax} function would return the index for the correct label. Similarly, the {\tt argmax} function applied to the predictions would return the index of the character with the highest probability calculated from the network. Finally we divide the number of correct labels on the total number of labels to get the percentage of correct predictions. This would produce a value of \(1.0\) if all labels were correctly predicted.

\begin{algorithm}
    \caption{Categorical accuracy
        \label{alg:categorical_accuracy}}
    \begin{algorithmic}[1]
        \Statex
        \Function{categorical\_accuracy}{$labels, predictions$}
            \Let{$count$}{$0$}
            \For{$i \gets 0 \textrm{ to } labels.length$}
                \If{$argmax(predictions, -1) = argmax(labels, -1)$}
                    \Let{$count$}{$count+1$}
                \EndIf
            \EndFor
        \State \Return{$count / labels.length$}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\red{Write about categorical crossentropy?}

%%=========================================

\section{Model Architecture}

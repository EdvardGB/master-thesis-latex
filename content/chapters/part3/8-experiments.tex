%% Experiments
%%=========================================

\chapter{Experiments}
\label{ch:experiments}
This chapter explains how the final experiments were carried out, as well as presenting the results for the experiments. These experiments and results are used for answering the research questions of this master thesis in the next chapter.

%%=========================================

\section{Approach}
Our experiments followed a traditional pattern with training, validation, and testing datasets. Carried out in iterations, our models were first fed the entire training set with the correct labels. After iterating the entire training set, we let the models predict on the validation set, recording their calculated accuracy and loss values. This alternating process goes on for many epochs until the model had reach a convergence where the loss for validation no longer makes significant changes. In addition, as this process goes on, we save the weights for the model with the best ever recorded validation loss value. As the alternating training and validation process is stopped, we load the weights for the best model, and we run that on the testing dataset, recording the final accuracy.

This is a pretty common approach, and has several benefits. Instead of only relying on a training set, we use a separate validation set. Unless these sets are very similar, or complete copies of each other, we know that the accuracy and loss on the validation set is in fact a result of generation outside what the model has learned. In addition, as we do not run the final test on the validation set, we do not only reuse the model that scored best on the validation process, but the model that is the result of the best generalization.

%%=========================================

\section{Dataset Details}
Datasets were created using the pipeline process explained in \ref{sec:construction_pipeline}. Our word list had a total of 356,719 unique words, populated by three open sourced word lists:

\begin{enumerate}
    \item sil.org\footnote{\url{http://www-01.sil.org/linguistics/wordlists/english/}} (109,582 words)
    \item Public GitHub repository dwyl/english-words\footnote{\url{https://github.com/dwyl/english-words}}, only the file ``words.txt" (354,985 words) 
    \item The English dictionary (only en\_US) for Apache OpenOffice\footnote{\url{http://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice}}. Preprocessed (39,908 words)
\end{enumerate}

The lists are filtered so only word consisting of letters are added. All words longer than one character is added to the final word set list. Finally, duplicated are removed.

%%=========================================

\section{Experiments}
This section presents the various experiments we ran.

\subsection{Accuracy on Datasets}
\label{sec:accuracy_on_datasets}
The first, and simplest, experiment did was to check how well our models could do prediction on datasets of various sizes. This was done to find out which model had the best prediction, as well as checking if certain models worked better under certain conditions.

For this experiment, three datasets were created. These sets had individual sets for training, validation, and testing, with the following sizes, and configurations:

\vspace{0.5cm}
\begin{minipage}{0.8\linewidth}
    \begin{description}
        \item[Small:]{1,000 training, 100 validation, 100 testing, max 10 chars}
        \item[Medium:]{5,000 training, 5,000 validation, 5,000 testing, max 15 chars}
        \item[Big:]{25,000 training, 2,500 validation, 2,500 testing, max 20 chars}
    \end{description}
\end{minipage}

\subsection{Handling of Two Fonts}
Special functionality was implement for the pipeline process to create datasets with multiple fonts. The pipeline chose one of \(N\) fonts, with an equal probability distribution. In this experiment we used the fonts Arial, which is also used in the other experiments, as well as Times New Roman. For each word in the training, validation, or test sets had a probability of \(\frac{1}{N}\) for choosing either Arial or Times New Roman.

The dataset used in this experiment had a training set of size \(10,000\) and validation and test tests of size \(1000\). The sets had a maximum length of 15 characters. The distribution between the two fonts are listed below:

\vspace{0.5cm}
\begin{minipage}{0.8\linewidth}
    \begin{description}
        \item[Training:]{Arial: 51.1\% (5,110), Times New Roman 49.0\% (4,900)}
        \item[Validation:]{Arial: 51.7\% (517), Times New Roman 48.3\% (483)}
        \item[Test:]{Arial: 51.2\% (512), Times New Roman: 48.8\% (488)}
    \end{description}
\end{minipage}

\subsection{Noise Handling}
In our final experiment we only used the best model architecture from the first test. We did this because this experiment required to run many tests to map how well the model handled noise. A special transformation handler was added that iterated over the constructed sets, and with a given probability could set a random bit value \(1\) or \(0\), implemented as in \ref{alg:random_noise}. The idea was to start with \(0\%\) noise, and then increase the noise threshold slowly while recording the results. This result could be plotted

\begin{algorithm}
    \caption{Apply random noise to input sequence
        \label{alg:random_noise}}
    \begin{algorithmic}[1]
        \Statex
        \Function{apply\_noise}{$input, threshold$}
            \For{$i \gets 0 \textrm{ to } input.length$}
                \If{$randint(0,100) \le threshold$}
                    \Let{$input[i]$}{$randint(0,1)$}
                \EndIf
            \EndFor
        \State \Return{$res$}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

%%=========================================


%% Experiments
%%=========================================

\chapter{Experiments}
\label{ch:experiments}
In this chapter, we present the final experiments that were carried out as a part of this thesis. Section \ref{sec:approach} presents the general experiment approach. We present the setup of our models in the preparation of the experiments in Section \ref{sec:setup}. Section \ref{sec:dataset_details} presents details about the datasets, and Section \ref{sec:experiments_details} presents the experiment details.

%%=========================================

\section{Approach}
\label{sec:approach}
Our experiments followed a traditional pattern with individual training, validation, and testing datasets. Carried out in iterations, our models were first fed the entire training set with the correct labels. After iterating the entire training set, we let the models predict on the validation set, recording their calculated accuracy and loss values. This alternating process went on until the model had reached a convergence where the loss for validation no longer made significant changes. As this process went on, we saved the weights for the model with the lowest validation loss value. When the alternating training and validation process was stopped, we loaded the weights for the best model, and tested that model on the testing dataset, recording the final accuracy.

%%=========================================

\section{Setup}
\label{sec:setup}
In this section, we present the setup, configurations, and choice of hyper-parameters in our three models. 

\begin{itemize}
    \item All three of our models used embedding to represent the input. This embedding was represented in a vector space with dimension 128. Using a higher dimension representation, like 1024 gave tiny improvements in results, but increased the computational time tremendously. A dimension size of 128 was a good trade-off, and made sense for our relatively small vocabulary. \cite{britz2017massive} concluded that using high embedding dimensions, such as 2048 achieved the best results, but only by a small margin, and that embeddings with smaller margins, such as 128 seem to have sufficient capacity to capture most of the necessary semantic information in their vocabulary which was significantly larger than ours. The encoder-decoder models also used embedding to represent the output.
    \item The embeddings were randomly initialized with an uniform distribution over the half-open interval \(\big[-\sqrt{3}, \sqrt{3}\big)\). As the embeddings were trainable, their placement in the vector space would change, which meant that the original placement was less important.
    \item Both LSTMs and GRUs have been used in neural machine translation architectures. We decided to use the LSTM cell in all our networks. 
    \item The LSTM units were based on the implementation of \cite{hochreiter1997long}. The inner activation function was hyperbolic tangent (see Equation \ref{eq:function_tanh} and plot in Figure \ref{fig:plot_function_tanh}), and the activation applied over the current timestep was a regular sigmoid function (see Equation \ref{eq:function_sigmoid} and plot in Figure \ref{fig:plot_function_sigmoid}). These configurations are pseudo-standard for the LSTM, although some variations exists.
    \item The weights in the LSTMs were initialized with ``Glorot uniform initializer'', also called ``Xavier uniform initializer'' \citep{glorot2010understanding}.
    \item All three models used groups of LSTMs. These had a depth of three. Smaller and bigger stacks affected results slightly, but a depth of three was a good trade-off between computational time and results.
    \item The forget gate in the LSTM module was initialized to 1.0. This was done to reduce the scale of forgetting at the beginning of training and is something that is recommended by \cite{zaremba2015empirical}.
    \item After each LSTM cell, we had a dropout layer, which randomly dropped units, along with their weights, by setting their values to 0 \citep{srivastava2014dropout}. The dropout rate after \emph{each} LSTM was 0.2, which meant that around 20\% of all output was ignored after each LSTM unit. The rate was a result of trial-and-error, but the rate did not significantly affect results in the range of \(\pm 0.1\).
\end{itemize}

\newpage
\begin{figure}[ht]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_tanh}
            \tanh{x} = \frac{\sinh{x}}{\cosh{x}} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
        \end{equation}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_sigmoid}
            \frac{1}{1+e^{-x}}
        \end{equation}
    \end{minipage}%
\end{figure}

\begin{figure}[ht]
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-5.0, xmax=5.0,
                    ymin=-1.5, ymax=1.5,
                    minor y tick num={5},
                    minor x tick num={3},
                    axis lines=center,
                    axis on top=true,
                    domain=-5:5,
                    ylabel=$y$,
                    xlabel=$x$,
                ]
                    \addplot[mark=none,draw=red,ultra thick]{tanh(\x)};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted hyperbolic tangent function}
        \label{fig:plot_function_tanh}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-6, xmax=6,
                    ymin=0, ymax=1,
                    xtick={-5, -3, -1, 1, 3, 5},
                    ytick={0, 0.5, 1},
                    axis lines=center,
                    axis on top=true,
                    domain=-6:6,
                    ylabel=$y$,
                    xlabel=$x$
                ]
                    \addplot[mark=none,draw=red,ultra thick]{1/(1+exp(-\x))};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted sigmoid function}
        \label{fig:plot_function_sigmoid}
    \end{minipage}%
\end{figure}

\subsection{Accuracy Metric}
Accuracy was calculated by computing the categorical accuracy for each label. This computation was done by comparing the categorical output of the prediction with the one-hot vector representing the correct label in the dataset.

\subsection{Loss Optimizer}
We tested various loss optimizers for our three models. Adaptive Moment Estimation (Adam) \citep{kingma2014adam} and Adadelta \citep{zeiler2012adadelta} are two loss optimizers, among many, that have been applied to encoder-decoder problems earlier \citep{cho2014properties, arik2017deep}. Our choice of optimizer was decided by trial-and-error and ultimately fell on Adam.

The Adam optimizer is a method that computes adaptive learning rates for each parameter. We kept the proposed default values of \(\beta_{1} = 0.9\), \(\beta_{2} = 0.999\), and \(\epsilon = 10^{-8}\). We used a learning rate of \(10^{-2}\), instead of the more common rate of \(10^{-3}\). We increased the learning rate as all models had a tendency to use many epochs to reach satisfactory progression.

\newpage
\section{Dataset Details}
\label{sec:dataset_details}
Datasets were created using the pipeline process explained in Section \ref{sec:preprocessor}. Our word list had a total of 356,719 unique words, populated by three open sourced word lists:

\begin{enumerate}
    \item sil.org\footnote{\url{http://www-01.sil.org/linguistics/wordlists/english/}} (109,582 words)
    \item Public GitHub repository dwyl/english-words\footnote{\url{https://github.com/dwyl/english-words}}, only the file ``words.txt'' (354,985 words) 
    \item The English dictionary (only en\_US) for Apache OpenOffice\footnote{\url{http://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice}}. Preprocessed (39,908 words)
\end{enumerate}

Only words consisting of actual letters were added, and the words had to have a minimal length of two characters. Duplicates were removed.

%%=========================================

\section{Experiments}
\label{sec:experiments_details}
This section presents the various experiments we conducted to achieve our research goal and answer our research questions.

\subsection{Accuracy on Datasets}
\label{sec:accuracy_on_datasets}
The first, and simplest experiment we did was to check how well our models could do prediction on datasets of various sizes. For this experiment, three datasets were created. These sets had the following sizes and configurations:

\begin{description}
    \item[Small:]{1,000 training, 100 validation, 100 testing, max 10 chars}
    \item[Medium:]{5,000 training, 500 validation, 500 testing, max 15 chars}
    \item[Big:]{25,000 training, 2,500 validation, 2,500 testing, max 20 chars}
\end{description}

Our goal with this experiment was to record how the recognition rates for each model were affected by the size of the datasets. This could potentially say something about the robustness of the models and how well they were able to learn from either a limited or big dataset. Our expectations for this experiment was that all three models should have relativity good accuracy across all datasets. We suspected that the accuracy would increase with the size of the datasets.

\subsection{Handling of Two Fonts}
The dataset used in this experiment had a training set of size 10,000 and validation and test sets of size 1000. The sets had a maximum length of 15 characters. The distribution between the two fonts are listed below:

\vspace{0.5cm}
\begin{minipage}{0.8\linewidth}
    \begin{description}
        \item[Training:]{Arial: 51.1\% (5,110), Times New Roman 49.0\% (4,900)}
        \item[Validation:]{Arial: 51.7\% (517), Times New Roman 48.3\% (483)}
        \item[Test:]{Arial: 51.2\% (512), Times New Roman: 48.8\% (488)}
    \end{description}
\end{minipage}

\begin{figure}[ht]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1\textwidth]{fig/experiments/SHORTCUTS_dpi.png}
    \caption{The word ``SHORTCUTS'' written in Arial Mono. Size 186x31}
\end{figure}

\begin{figure}[ht]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1\textwidth]{fig/experiments/UPLIFTER_dpi.png}
    \caption{The word ``UPLIFTER'' written in Times New Roman. Size 180x31}
\end{figure}

Our goal with this experiment was to record how the individual models were able to handle input written in more than one font. We expected the two encoder-decoder models to perform better in this experiment than the {\tt VecRep} model but did not exclude that the {\tt VecRep} model could reach satisfactory results either.

\subsection{Noise Handling}
In the third experiment, we added more noise increasingly to the input data. This experiment was conducted to record how resilient and robust the model was to noise in the data. For this experiment, we only used the best performing model. The amount of noise added for each test was based on results underway, as we did not know how the model would respond to the noise beforehand.

The goal of this experiment was to record how the accuracy would decrease while the amount of noise increased, and we expected the model to handle noise to a degree we found satisfactory.

\subsection{Stress Test}
The final experiment was a ``stress test'' where we increased the complexity of the task in various ways. For this experiment, we used a total of five fonts:

\begin{itemize}
    \item Arial (monospaced)
    \item Times New Roman
    \item Courier
    \item Georgia
    \item Verdana
\end{itemize}

These fonts are variants of serif fonts (Georgia and Times New Roman), sans-serif fonts (Arial, Verdana), and monospaced fonts (Arial monospaced, Courier).

This experiment had words written in both upper-cased and lower-cased letters, in contrast to the other experiments which only had upper-cased words. The other experiments, having exclusively upper-cased letters, had a total of 27 classes (A-Z plus an ``empty'' class), whereas this experiment would have a total of 53 classes (A-Z, a-z, and the ``empty'' class). The ``empty'' class was used to pad the values as the words had different lengths. The datasets had a size of 50,000, 5,000, and 5,000 for the training, testing, and validation sets respectively. We also added a noise factor of 10\%. Some of the generated words are listed in Appendix \ref{ch:example_words}.

The goal of this experiment was primarily to see just how far we could go with the models. We had little information on how the two encoder-decoder models would perform under these conditions. Our general expectations for this experiment was for the {\tt EncDecAtt} model to outperform the {\tt EncDecReg} model.
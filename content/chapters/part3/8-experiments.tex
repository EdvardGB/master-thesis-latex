%% Experiments
%%=========================================

\chapter{Experiments}
\label{ch:experiments}
In this chapter we explain the final experiments that was carried out as a part of this thesis. Section \ref{sec:approach} presents the general experiment approach. Section \ref{sec:setup} presents the setup of our models in preparation of the experiments, and Section \ref{sec:experiments_details} presents the experiment details.

%%=========================================

\section{Approach}
\label{sec:approach}
Our experiments followed a traditional pattern with individual training, validation, and testing datasets. Carried out in iterations, our models were first fed the entire training set with the correct labels. After iterating the entire training set, we let the models predict on the validation set, recording their calculated accuracy and loss values. This alternating process goes on until the model had reach a convergence where the loss for validation no longer makes significant changes. In addition, as this process goes on, we save the weights for the model with the best ever recorded validation loss value. As the alternating training and validation process is stopped, we load the weights for the best model, and we run that on the testing dataset, recording the final accuracy.

This is a pretty common approach, and has several benefits. Instead of only relying on a training set, we use a separate validation set. Unless these sets are very similar, or complete copies of each other, we know that the accuracy and loss on the validation set is in fact a result of generation outside what the model has learned. In addition, as we do not run the final test on the validation set, we do not only reuse the model that scored best on the validation process, but the model that is the result of the best generalization.

%%=========================================

\section{Setup}
\label{sec:setup}
In this section we present the setup, configurations, and choice of hyper-parameters in our three models. 

\begin{itemize}
    \item All three of our models used embedding. This embedding was represented in a vector space with dimension 128. Using a higher dimension representation, like 1024 gave tiny improvements in results, but increased the computational time tremendously. A dimension size of 128 was a good trade-off, and made sense for our relatively small vocabulary.
    \item The embeddings were randomly initialized with an uniform distribution over the half-open interval \(\big[-\sqrt{3}, \sqrt{3}\big)\). As the embeddings are trainable, their placement in the vector space will change, which means that the original placement is less important.
    \item The LSTM units used were implemented based on \citep{hochreiter1997long}. The inner activation function was hyperbolic tangent (see Equation \ref{eq:function_tanh} and plot in Figure \ref{fig:plot_function_tanh}), and the activation applied over the current timestep was a regular sigmoid function (see Equation \ref{eq:function_sigmoid} and plot in Figure \ref{fig:plot_function_sigmoid}). These configurations are pseudo-standard for the LSTM, although some variations exists.
    \item The weights in the LSTMs were initialized with ``Glorot uniform initializer", also called ``Xavier uniform initializer" \citep{glorot2010understanding}.
    \item All three models used groups of LSTMs. These were stacked sequentially and had a depth of three. Smaller and bigger stacks affected results slightly, but a depth of three was a good trade-off between computational time and results.
    \item The forget gate in the LSTM module was initialized to 1.0. This is done in order to reduce the scale of forgetting at the beginning of training, and is something that is recommended in \citep{zaremba2015empirical}.
    \item After each individual LSTM unit, we had a dropout layer, which randomly dropped units, along with their weights, by setting their values to 0 \citep{srivastava2014dropout}. The dropout rate after \emph{each} LSTM was 0.2, which meant that around 20\% of all units were ignored after each individual LSTM unit. The rate was a result of trial-and-error, but the rate did not significantly affect results in the range of \(\pm 0.1\).
\end{itemize}

\begin{figure}[ht]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_tanh}
            \tanh{x} = \frac{\sinh{x}}{\cosh{x}} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
        \end{equation}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{equation}\label{eq:function_sigmoid}
            \frac{1}{1+e^{-x}}
        \end{equation}
    \end{minipage}%
\end{figure}

\begin{figure}[ht]
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-5.0, xmax=5.0,
                    ymin=-1.5, ymax=1.5,
                    minor y tick num={5},
                    minor x tick num={3},
                    axis lines=center,
                    axis on top=true,
                    domain=-5:5,
                    ylabel=$y$,
                    xlabel=$x$,
                ]
                    \addplot[mark=none,draw=red,ultra thick]{tanh(\x)};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted hyperbolic tangent function}
        \label{fig:plot_function_tanh}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \captionsetup{justification=centering}
        \resizebox{0.8\textwidth}{4cm}{
            \begin{tikzpicture}[framed]
                \begin{axis}[
                    xmin=-6, xmax=6,
                    ymin=0, ymax=1,
                    xtick={-5, -3, -1, 1, 3, 5},
                    ytick={0, 0.5, 1},
                    axis lines=center,
                    axis on top=true,
                    domain=-6:6,
                    ylabel=$y$,
                    xlabel=$x$
                ]
                    \addplot[mark=none,draw=red,ultra thick]{1/(1+exp(-\x))};
                \end{axis}%
            \end{tikzpicture}%
        }
        \caption{Plotted sigmoid function}
        \label{fig:plot_function_sigmoid}
    \end{minipage}%
\end{figure}

\subsection{Accuracy Metric}
Accuracy was calculated by computing the categorical accuracy for each label. This was done by comparing the categorical output of the prediction with the one-hot vector representing the correct label in the dataset.

\subsection{Loss Optimizer}
We tested various loss optimizers for our three models. Adaptive Moment Estimation (Adam) \citep{kingma2014adam} and Adadelta \citep{zeiler2012adadelta} are two loss optimizers, among many, that have been applied to encoder-decoder problems earlier \citep{cho2014properties, arik2017deep}. Our choice of optimizer was decided by trial-and-error and ultimately fell on Adam.

The Adam optimizer is a method that computes adaptive learning rates for each parameter. We kept the proposed default values of \(\beta_{1} = 0.9\), \(\beta_{2} = 0.999\), and \(\epsilon = 10^{-8}\). We used an learning rate of \(10^{-2}\), instead of the more common rate of \(10^{-3}\). We increased the learning rate as all models had a tendency to use many epoch to reach satisfactory progression.

\section{Dataset Details}
Datasets were created using the pipeline process explained in Section \ref{sec:preprocessor}. Our word list had a total of 356,719 unique words, populated by three open sourced word lists:

\begin{enumerate}
    \item sil.org\footnote{\url{http://www-01.sil.org/linguistics/wordlists/english/}} (109,582 words)
    \item Public GitHub repository dwyl/english-words\footnote{\url{https://github.com/dwyl/english-words}}, only the file ``words.txt" (354,985 words) 
    \item The English dictionary (only en\_US) for Apache OpenOffice\footnote{\url{http://extensions.openoffice.org/en/project/english-dictionaries-apache-openoffice}}. Preprocessed (39,908 words)
\end{enumerate}

The lists are filtered so only word consisting of letters are added. All words longer than one character is added to the final word set list. Finally, duplicated are removed.

%%=========================================

\section{Experiments}
\label{sec:experiments_details}
This section presents the various experiments we conducted in response to our research goal and research questions.

\subsection{Accuracy on Datasets}
\label{sec:accuracy_on_datasets}
The first, and simplest, experiment did was to check how well our models could do prediction on datasets of various sizes. This was done to find out which model had the best prediction, as well as checking if certain models worked better under certain conditions.

For this experiment, three datasets were created. These sets had individual sets for training, validation, and testing, with the following sizes and configurations:

\vspace{0.5cm}
\begin{minipage}{0.8\linewidth}
    \begin{description}
        \item[Small:]{1,000 training, 100 validation, 100 testing, max 10 chars}
        \item[Medium:]{5,000 training, 5,000 validation, 5,000 testing, max 15 chars}
        \item[Big:]{25,000 training, 2,500 validation, 2,500 testing, max 20 chars}
    \end{description}
\end{minipage}

\subsection{Handling of Two Fonts}
The dataset used in this experiment had a training set of size 10,000 and validation and test tests of size 1000. The sets had a maximum length of 15 characters. The distribution between the two fonts are listed below:

\vspace{0.5cm}
\begin{minipage}{0.8\linewidth}
    \begin{description}
        \item[Training:]{Arial: 51.1\% (5,110), Times New Roman 49.0\% (4,900)}
        \item[Validation:]{Arial: 51.7\% (517), Times New Roman 48.3\% (483)}
        \item[Test:]{Arial: 51.2\% (512), Times New Roman: 48.8\% (488)}
    \end{description}
\end{minipage}

\subsection{Noise Handling}
In the third experiment we added increasingly more noise to the input data. This experiment was conducted to record how resilient and robust the model was to noise in the data. For this experiment we only used the best performing model. The amount of noise added for each test was based on results underway, as we did not know how the model would respond to the noise beforehand.

\subsection{Stress Test}
The final experiment was a ``stress test" where we increased the complexity of the task in various ways. For this experiment, we used a total of five fonts:

\begin{itemize}
    \item Arial (monospaced)
    \item Times New Roman
    \item Courier
    \item Georgia
    \item Verdana
\end{itemize}

These fonts are variants of serif fonts (Georgia and Times New Roman), sans-serif fonts (Arial, Verdana), and monospaced fonts (Arial monospaced, Courier).

This experiment had words written in both upper-cased and lower-cased letters, in contrast to the other experiments which only had upper-cased words. The other experiments, having exclusively upper-cased letters, had a total of 27 classes (A-Z plus an ``empty" class), whereas this experiment would have a total of 53 classes (A-Z, a-z, and the ``empty" class). The datasets had a size of 50,000, 5,000, and 5,000 for the training, testing, and validation sets respectively. We also added a noise factor of 10\%. We have included some of the generated words in Appendix \ref{ch:example_words}.
%% Conclusion and future work
%%=========================================

\chapter{Conclusion and Future Work}
\label{ch:conclusion}
Section \ref{sec:conclusion} presents the final conclusions drawn from the results presented and discussed in the previous chapter. We evaluate our research contributions in Section \ref{sec:contributions_final}, and in Section \ref{sec:future_work} we present ideas for potential future work and improvements.

\section{Conclusion}
\label{sec:conclusion}
As a result of research, we have developed two models based on the encoder-decoder framework that gave satisfying results on the experiments we carried out. The {\tt EncoDecAtt} model was able to correctly classify and differentiate between 27 classes with an accuracy of 92\%, 97\%, and almost 99\% for experiments carried out on increasingly larger datasets. The model was also able to classify a problem with 53 classes with an accuracy of 88\% under challenging conditions.

We have shown that both the encoder-decoder models were able to handle signature sequence ambiguity, although some letters were repeatedly misclassified as other letters. We have also shown that the {\tt EncDecAtt} was able to handle input with two fonts without nearly any reduced accuracy, while the {\tt EncDecReg} model had more trouble with this. Lastly, we have shown that the {\tt EncDecAtt} model was robust to noise. For reasonable low amounts of noise, the accuracy remained high, but quickly deteriorated once the amount of noise increased.

The two models built and presented in this thesis are built on state-of-the-art technologies, and uses state-of-the-arts approaches in the area of (neural) machine translation. These models illustrates the power of the encoder-decoder framework and proves how this framework is able to handle input and output with unknown alignments with high precision.

We have concluded that we met the research goal of developing a model that was able to use signature sequences to recognize machine-written letters and words on the basis of the experiments we have conducted and the results they gave.

%%=========================================

\section{Contributions}
\label{sec:contributions_final}

The encoder-decoder framework has attracted much attention, and despite the approach only dating back to 2014 has become almost ``de facto standard" in various sequence-to-sequence mapping related problems. The model has, since its recent inception, been used in countless models, and the framework has played an important role in achieving various state-of-the-art results in areas such as machine translation, computer vision, and speech recognition. These achievement may undermine some of the results we have achieved in the work in this thesis to some degree, as the framework has already been proven countless times to handle much more complex problems. The research contribution this thesis does not revolve around proving just how powerful the encoder-decoder framework is. Instead, we have demonstrate how the encoder-decoder is able to handle a problem with characteristics like ours. We have already state how our problem is both similar and dissimilar to traditional translation problems in Section \ref{sec:translation}.

The main contribution in therefore the exploration in using the encoder-decoder framework and the attention mechanism. We have shown how the framework handles input and output that have different widths. Some of the datasets used in our experiments had a ration between input and output of nearly \(7 : 1\), which is more than most traditional translation tasks. We have also illustrated how the attention mechanism aligns with the same ratio between input and output. We have also shown how the attention mechanism acts differently depending on the ambiguity of the input data.

As our data had a strict ordering from start to end, and there was no alterations in the ordering of neither the input values, nor the output values. We have shown that the encoder-decoder models have successfully learned not to swap or reorder output values, something that is common when translating between two spoken languages. Furthermore, we have also illustrated how the encoder-decoder framework encodes the input information, and how noise affects this process. Lastly, we have explored how the mechanism of feeding back previous output affects the decoder module, and how abusing this mechanism results in incorrect predictions.

%%=========================================

\section{Future Work}
\label{sec:future_work}
The results archived by the {\tt EncDecAtt} model is, under certain conditions, nearing perfect. It could be interesting to see if the model is able to improve even further by using more than one signature sequence. This could be done by capturing two sequences at different heights, for example as illustrated in Figure \ref{fig:thesis-signature-comparison}. Doing this could also have the potential to eliminate ambiguity. Without ambiguity, the model could be able to recognize the input perfectly. This opens up possibilities to use the approach as a potential lossless compression algorithm.

We have focused our experiments in this thesis on recognizing single words. It would be interesting to see if a similar approach to ours could be applied to multiple words, or even full sentences. This would require the model to handle much longer input data, as well as sequences that are broken up into separate words.

There are also possibilities left untouched related to both the encoder-decoder framework, and the attention mechanism. Numerous variants and improvements have been proposed for both of them, and the two encoder-decoder models implemented in this thesis barely scratches the surface for what is possible.
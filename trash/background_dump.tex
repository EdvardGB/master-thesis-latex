%% Background Theory
%%=========================================

\chapter{Background Theory}
\label{ch:background}
In this chapter we will look at theory relevant to the the underlying structure of our problem.

%%=========================================

\section{Understanding Our Problem}
Understanding what type of problem we are trying to solve is important. While many problems may seem similar in nature, small factors can influence how a problem is best solved. Some factors even influence the problem so much that it becomes a completely different one. This chapter investigates what type of problem we are trying to solve.

The next section evaluates what data we want to give to our model, and what results we hope the model outputs. Section \ref{sec:classification_explanation} first presents the concept of classification, and section \ref{sec:problem_dissimilarities} explains how our problem is different from it.

%%=========================================

\section{Evaluating Problem Input and Output}
In our particular problem we have a input that is a matrix or a vector of binary data. The binary input denotes the color of a particular pixel at the given location in our ``signature". We want the output to be what character(s) the matrix or vector is made out of. For example, if our characters are the upper cased letters in the English language, we could want our output to be in the range 1 to 26, denoting A to Z. See \ref{eq:input_output_example} for example input and output for the word ``ALLIED".

\begin{equation}
    \label{eq:input_output_example}
    \begin{aligned}
       \vec{rawInput}        &= \lbrack 4W, 3B, 7W, 3B, 8W, 3B, 18W, 3B, 23W, 3B, 13W, 14B, 6W, 3B, 10W, 3B \rbrack \\
       \vec{encodedInput}    &= \lbrack 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 8, 9, 1, 10, 1 \rbrack \\
       \vec{output}          &= \lbrack 1, 12, 12, 9, 5, 4 \rbrack \\
       word                  &= \text{ALLIED}
    \end{aligned}
\end{equation}

Note that in example \ref{eq:input_output_example} we encoded our input to whole integers. The encoding is done by assigning unique numbers to each input value. In our case, we assigned numbers ranked by frequency. `3B` occurs seven times in the raw output, while all the other values have a frequency of one.

\subsection{Input Format}
Our input has the feature that they form a sequence. Both the values in the sequence, and the ordering of the sequence is crucial for prediction. This is fundamentally different from other problems such as traditional image recognition, where the exact location of a pattern is not important.

Because the input forms a sequence, it is important that the entire sequence is read, and that we do not cut the sequence off at the end, removing important information that we need. Truncating or ignoring a values in the input sequence would result in mislearning. Instead of our model correctly identify subsequences that result i a single output value, the model attempts to find patters in the data that is not there. This would cripple the model and the overall accuracy may suffer due to contradicting sequences.

We do also lack the concept of ``stop words" in our problem. Example \ref{eq:input_stop_words} illustrates an input with stop words, denoted as $\pi$. Using a stop word would make the problem easier to solve, as we would know within which boundaries each character resides. In example \ref{eq:input_stop_words}, we have placed the stop word right before the beginning of a new character, instead of just the barriers of the character itself. This could potentially reduce ambiguity as we would now know for a fact that the letter I, if followed by an E, would always be the subsequence $[1, 7]$. Instead of relying on stop words, we want our model to find a pattern in the sequence data that makes sense based on the corresponding output. This pattern, if correctly predicted,
would not need explicit stop words, as the model should be able to find them implicitly.

\begin{equation}
    \label{eq:input_stop_words}
    \begin{aligned}
       \vec{encodedInput}        &= \lbrack 1, 6, \pi, 1, 7, \pi, 8, \pi \rbrack \\
       word                      &= \text{LIE}
    \end{aligned}
\end{equation}

\subsection{Output Format}
As with the input, our output also form a sequence. Both the values in the sequence, and the ordering of the sequence, is important. The words `HELLO` and `HLLOE` contains the same letters, but have a different meanings. Understanding that both the individual values in the output sequence, as well as the placement of each of them, is important for understanding how our problem is different from many others. The structure of the output is elaborated more in section \ref{sec:meaning_of_output}.

%%=========================================

\section{Classification}
\label{sec:classification_explanation}
Classification has two distinct meanings. Classification under supervised learning is the task of finding a rule that can be used classify an observation in one more classes, when we know the classes beforehand. Classification under unsupervised learning is often called clustering. This is the task of given a set of unlabeled data, we want to find the existence of classes or clusters in the data. In our particular problem, we know the labels for our observations, so our problem falls in the task of supervised learning.

The goal of classification is, as previously stated, to create a function that can be take an observation and give it one or more classes. The classifier is ``trained" on a set of data where we know the correct labels for each of the observations. With this training, the classifier should be able to ``learn" how to do classifications on its own. The important factor here is that the classifier should not just ``remember" earlier observations and simply return the correct answers, it should learn how to calculate the answers instead. 

\begin{equation}
    \label{eq:key_value_map}
    h: K \mapsto V
\end{equation}

In example \ref{eq:key_value_map} $K$ denotes the keys, and $V$ denotes the values, our input and output respectfully. The example is a simple look up, similar to data structures such as hash maps. Unless the key $k$ has an exact match in $K$, it has no way of returning any value. Instead of a look up function like this, we want a function as in \ref{eq:simple_function}, where the function $f(x)$ does some calculation to derive at the result $y$.

\begin{equation}
    \label{eq:simple_function}
    f(x) = y
\end{equation}

\subsection{Example of Classification}
\label{sec:classification-example}
Image classification is a common example of classification. A model is given the image data, and a set of categories, and its job is to identify in which category, or categories, the image belongs.

Figure \ref{fig:facebook-classification-example} illustrates classification using computer vision software. It contains an photo of Hovedbygget at Gl√∏shaugen. The photo was uploaded to Facebook, and classified by their computer vision software. Using the Show Facebook Computer Vision Tags Chrome extension\footnote{\url{https://github.com/ageitgey/show-facebook-computer-vision-tags}}, we can see the categories in the top right corner of the photo.

\begin{figure}[H]
    \label{fig:facebook-classification-example}
    \centering
    \includegraphics[width=0.7\textwidth]{fig/chapter4/facebook_classification_example.png}
    \caption{Personal photo of Hovedbygget, classified by Facebook}
\end{figure}

When Facebook classified my image, their software took all the categories they have, and gave each of them a probability based on what the software had seen in their training set. All the categories above a given threshold were returned and assigned to my image. 

\subsection{Dissimilarities}
\label{sec:problem_dissimilarities}
To understand our problem behaves, dissimilarities to typical classification is presented to illustrate this.

\subsubsection{Meaning of Output}
\label{sec:meaning_of_output}
As illustrated with the image classification of Figure \ref{fig:facebook-classification-example}, typical classification algorithms does not return the output the way we need in our problem. As already stated, the classifier in this case returned all the classes above a given threshold given its probability based on earlier observations in its training set. While the categories can be sorted by their probability, there is no other concept of ordering among them. Algorithm \ref{alg:classification_probability_example} illustrates how a probability model could handle its output. The algorithm inputs a list of probabilities, which in this case would be the probabilities given to each category by the classifier, and outputs a list of tuples consisting of the probabilities and their index values for categories above a given threshold.

In our problem the ordering is, as already emphasised, crucial. Words are not the same when their letters are reordered. While we could look at the entire output vector as a single category, but that would require one category for each word, making it not only very insufficient, but would also make it impossible to classify new, unseen words.

\begin{algorithm}
    \caption{Return classified categories with probabilities and indexes above a given threshold
        \label{alg:classification_probability_example}}
    \begin{algorithmic}[1]
        \Statex
        \Function{return\_categories}{$X, t$}
            \Let{$res$}{$[ ]$} \Comment{Initialize empty list}
            \For{$i \gets 1 \textrm{ to } X.length$}
                \If{$X[i] \ge t$}
                    \Let{$res$}{$(X[i], i)$} \Comment{Add tuple (probability, index) to list}
                \EndIf
            \EndFor
        \State \Return{$res$}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

%%=========================================

\section{Sequence Classification}
Sequence classification is a type of problem that falls in the general task of classification. This particular problem however, has a characteristic that makes it more specialized than many other classification tasks.

The characteristics lies in how the classifier interprets the input. Image classification is often called contextual, which means that the classification is based on the relationship between nearby data. For image classification, this makes sense as a single pixel itself does not make much sense, but given a group of pixels, we can try to recognize certain traits or patterns. In sequence classification, we must evaluate the entire sequence as a whole. Considering just the nearby values would not be sufficient in trying to make sense of a input sequence. \red{Images are dense, text is sparse}

A typical sequence classification example is classification of reviews and an associated score, which can be either a number, or a positive/negative value. The reviews contains the text from the reviewer, which will typically contain either positive or negative worded words. Table \ref{table:sequence-classification-illustration} contains illustrative reviews. Finding patterns in the reviews, you can not evaluate words individually. Instead you must see the entire context in which the words are used.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|l|}
        \hline 
        \textbf{Review}                                                    & \textbf{Recommends} \\ \hline
        The food tasted \textbf{terrible} and the place was \textbf{dirty} & No                  \\ \hline
        Food was \textbf{expensive}, and the waiter was \textbf{lazy}      & No                  \\ \hline
        \textbf{Better} than all the other restaurants                     & Yes                 \\ \hline
        You are \textbf{better} off eating at home                         & No                  \\ \hline
    \end{tabular}
    \label{table:sequence-classification-illustration}
    \caption{Illustrative review data}
\end{table}

Sequence classification is closer to the type of model we will need to solve our problem. However, as it falls in the category of typical classification tasks, it has the same problem as image classification - that it returns classes without any ordering.

%%=========================================

\section{Sequence to Sequence Learning}
Sequence to sequence learning is a type of machine learning that falls outside the typical classification tasks. In sequence to sequence learning, the input sequence is evaluated, and the output is another sequence. The task of the model is to find patterns in the input sequence that returns the corresponding output sequence. This is the type of model that is capable of solving a problem like ours.

\begin{equation}
    \begin{aligned}
        f(\vec{input}) = \vec{output}
    \end{aligned}
\end{equation}

\subsection{Natural Language Processing}
Using 


\iffalse
Alan Turing published his paper ``Computer Machinery and Intelligence" in 1950. In this paper he explained a game he called ``imitation game", in which a machine tries to impersonate a person via real-time written conversation. An interrogator communicates with a machine and a human and is trying to decide which is which. Turing suggested considering if a machine would win the imitation game, rather than asking the question of ``can machines think?". The game, as carried out via written text, means the computer have to be able to convert language text into a language it understand itself \citep{turing1950computing}.
\fi